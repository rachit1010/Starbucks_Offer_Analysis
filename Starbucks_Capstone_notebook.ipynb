{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record\n",
    "\n",
    "**Note:** If you are using the workspace, you will need to go to the terminal and run the command `conda update pandas` before reading in the files. This is because the version of pandas in the workspace cannot read in the transcript.json file correctly, but the newest version of pandas can. You can access the termnal from the orange icon in the top left of this notebook.  \n",
    "\n",
    "You can see how to access the terminal and how the install works using the two images below.  First you need to access the terminal:\n",
    "\n",
    "<img src=\"pic1.png\"/>\n",
    "\n",
    "Then you will want to run the above command:\n",
    "\n",
    "<img src=\"pic2.png\"/>\n",
    "\n",
    "Finally, when you enter back into the notebook (use the jupyter icon again), you should be able to run the below cell without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'livelossplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-366-f93e2cd14b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlivelossplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'livelossplot'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import SVG\n",
    "import livelossplot\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Exploration (Portfolio Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing top5 rows from portfolio data\n",
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing number of rows and columns in portfolio dataframe\n",
    "print(\"portfolio data: Rows = {0}, Columns = {1}\".format(str(portfolio.shape[0]), str(portfolio.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing details in form of quantile, max, min , mean, standard deviation etc\n",
    "portfolio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing datatype info about attributes of portfolio dataset\n",
    "portfolio.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null values\n",
    "portfolio.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring different attributes under channel column in portfolio dataset\n",
    "portfolio.channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Portfolio Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we need to separate the values of channels attribute and create separate table for values of channels \n",
    "# I will apply the concept of dummy columns, I will rename the column \"id\" as \"offer_id\", as its part of portfolio's offer.\n",
    "\"\"\"\n",
    ".apply(pd.Series) converts the series of lists to a dataframe\n",
    ".stack() puts everything in one column again (creating a multi-level index)\n",
    "pd.get_dummies( ) creating the dummies\n",
    ".sum(level=0) for remerging the different rows that should be one row (by summing up the second level, only keeping the original level (level=0))\n",
    "\"\"\"\n",
    "df1= portfolio.copy()\n",
    "\n",
    "dummy = pd.get_dummies(df1.channels.apply(pd.Series).stack()).sum(level=0)\n",
    "df1 = pd.concat([df1, dummy], axis=1)\n",
    "df1 = df1.drop(columns='channels')\n",
    "df1 = df1.rename(columns={'id':'offer_id'})\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Data Exploration (Profile Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing top5 rows of profile data\n",
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Row and Column detail about profile data\n",
    "profile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile, min, max etc info about profile data\n",
    "profile.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all values under age attribute of profile dataset\n",
    "profile['age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype information of profile attributes\n",
    "profile.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null values\n",
    "profile.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Profile data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the type of became_member_on attribute, as it contains value in the format of yy/mm/dd\n",
    "type(profile.became_member_on[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating copy of dataframe for further implementaion, we need to change the type of \"became_member_on\" attribute \n",
    "# We will change the column name \"id\" to \"customer_id\"\n",
    "df2 = profile.copy()\n",
    "\n",
    "df2['became_member_on'] = pd.to_datetime(df2['became_member_on'], format='%Y%m%d')\n",
    "\n",
    "df2 = df2.rename(columns={'id':'customer_id'})\n",
    "df2.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  From above it looks like, that all null values belong to \"age=118\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the above fact\n",
    "df2[df2['age']== 118].drop(['became_member_on', 'customer_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross checking the type of \"became_member_on\" attribute\n",
    "type(df2.became_member_on[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Exploration (Transcript Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transcript data \n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing rows and columns of transcript.json file\n",
    "transcript.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing quantile, mean, max,min etc about transcript file\n",
    "transcript.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying information about data type of transcript attributes\n",
    "transcript.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the type of value and number of values against offer id\n",
    "transcript['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the type of event and number of those event\n",
    "transcript['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null value. if it is present in transcript data set\n",
    "transcript.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Cleaning Transcript Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First we will create another copy of transcript dataset like profile and portfolio.\n",
    "Will chnage the column name 'person' to 'customer_id'\n",
    "Convert the column 'event' into 4 different columns, based on their types. Will apply the concept of dummy.\n",
    "\n",
    "\"\"\"\n",
    "df3 = transcript.copy()\n",
    "\n",
    "df3['event'] = df3['event'].str.replace(' ', '-')\n",
    "\n",
    "df3 = df3.rename(columns={'person':'customer_id'})\n",
    "dummy = pd.get_dummies(df3['event'])\n",
    "df3 = pd.concat([df3, dummy], axis=1 )\n",
    "df3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is picked from https://stackoverflow.com/questions/60106364/syntaxerror-invalid-syntax-when-using-lambda-function-in-pandas-apply\n",
    "df3['offer_id'] = [[*i.values()][0]if [*i.keys()][0] in ['offer id','offer_id'] else None for i in df3.value]\n",
    "df3['amount'] = [np.round([*i.values()][0], decimals=2)if [*i.keys()][0] == 'amount' else None for i in df3.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the value column \n",
    "df3 = df3.drop(columns='value')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now We are done with data exploration and cleaning for all three datasets. Now as per need we will combine the data from all three datasets and draw visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on common attributes of all three datasets, we will try to merge the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First will start to merge transcript and profile data on 'cutomer id'\n",
    "df_merge= pd.merge(df3, df2, on='customer_id')\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the portfolio dataset with df_merge dataframe\n",
    "df = pd.merge(df_merge, df1, on='offer_id', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try some graph based on the findings of so far, before that lets find mean value\n",
    "df['income'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distplot plot based on time attribute \n",
    "# This code is picked from https://indianaiproduction.com/seaborn-histogram-using-seaborn-distplot/\n",
    "\"\"\"\n",
    "Time attribute is related to offer_id, customer_id and other important attributes directly, so its \n",
    "important here to check maximum number of offers provided for particular category, as of now I am not \n",
    "defining 'time' attribute with respect to other attribute. I am just considering 'time' attribute alone.\n",
    "\"\"\"\n",
    "plt.figure(figsize=(16,9))\n",
    "sns.set()\n",
    " \n",
    "# hist keyword argument to change hist format\n",
    "sns.distplot(df[\"time\"],\n",
    "            hist_kws = {'color':'#DC143C', 'edgecolor':'#aaff00',\n",
    "                       'linewidth':5, 'linestyle':'--', 'alpha':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['offer_id'].value_counts().plot.barh(title=' Distribution of offer_ids')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows and columns in df dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed information about df dataframe in terms of quantile, mean, deviation, min, max etc\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data type information about the attributes of dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['offer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all unique values of offer_id\n",
    "offer_id = df['offer_id'].unique()\n",
    "offer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the offer_id values into dictionary form\n",
    "offer_dict = pd.Series(offer_id ).to_dict()\n",
    "offer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_dict = dict([(value, key) for key, value in offer_dict.items()]) \n",
    "offer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the offer_id with offer_dict, in order to get  \n",
    "df['offer_id'] = df['offer_id'].map(offer_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['offer_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = df['event'].unique()\n",
    "event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to make compare and visualize all factors related to company promotions, like offers, event, we need to create co relation \n",
    "# between all factors, so that we can create more clear visualization. Here I am creating separate column, called event_id\n",
    "# which is mapped according to event attribute.\n",
    "event_dict = pd.Series(event_id).to_dict()\n",
    "\n",
    "event_dict = dict([(value, key) for key, value in event_dict.items()]) \n",
    "df['event_id'] = df['event'].map(event_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets cross check structure of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the dataframe into csv data\n",
    "df.to_csv('data/data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring data file\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_bar_plot(data, colName, pltTitle, pltfigsize=[5,5], bInvertYAxis=True, fntSize=12, fntWeight='bold'):\n",
    "    \"\"\"\n",
    "    Creates a horizontal bar plot with counts of categories\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: The dataframe with categories.\n",
    "    colName: The name of the column containing the categories.\n",
    "    pltTitle: The plot title.\n",
    "    pltfigsize: The figure size of the plot (default is [5,5]).\n",
    "    bInvertYAxis: A flag to indicating whether to invert the Y-Axis or not.\n",
    "    fntSize: The font size of text (default to 12 point font size)\n",
    "    fntWeight: The weight of font (default to 'bold', it could be 'italic')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=pltfigsize)\n",
    "    fig, ax = plt.subplots()    \n",
    "    y_counts = data[colName].value_counts()\n",
    "    if bInvertYAxis == True:\n",
    "        y_counts.plot(kind='barh').invert_yaxis()\n",
    "    else:\n",
    "         y_counts.plot(kind='barh')   \n",
    "    for i, v in enumerate(y_counts):\n",
    "        ax.text(v, i, str(v), color='black', fontsize=fntSize, fontweight=fntWeight)\n",
    "        plt.title(pltTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling number of event categories as per cleaned data\n",
    "horizontal_bar_plot(data, 'event', \"Break-down of available event in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : From above graph it is clear that only approx half offers reach to audience and out of that only 1/3 offers are completed by customers. Many offers dont even get seen after receiving.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_bar_plot(data, 'offer_type', \"Break-down of available offer types in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Bogo and discount offers have similar distributions, but Bogo comes on top. Thats why many patterns are coming based on Bogo in market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using histogram trying to pull outliers\n",
    "data.age.hist(bins = 20, grid=True)\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Age Group Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using box plot trying to pull outliers\n",
    "sns.boxplot(profile['age'], width=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation : \n",
    "##### Based on above two plots (histogram & Boxplot), we can see that age>100 is present , which is not possible. So will consider values above \n",
    "#### 100 as outlier. Age group between 47-62 use application the most. 63-71 age group uses the Starbuck app second most, as any\n",
    "#### one may think here that younger age group must be using the app mostly. But this is not true here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using histogram trying to pull outliers\n",
    "data.income.hist(bins = 20, grid=True)\n",
    "plt.xlabel('Income Range')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Income Range Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : Average income is 65000-74000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Male gender population  : {} % \" .format(str(data.gender.value_counts()[0] / data.shape[0]*100)))\n",
    "print(\"Female gender population  : {} % \" .format(str(data.gender.value_counts()[1] / data.shape[0]*100)))\n",
    "print(\"Other gender population  : {} %\" .format(str(data.gender.value_counts()[2] / data.shape[0]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_bar_plot(data, 'gender', \"Break-down of gender types in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : Male is playing major role towards using Starbucks app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"offer_type\", hue= \"gender\", data=data)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Gender distribution as per offer_type')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Offer_Type')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : As we can see here , that Male are using the all types of offers mostly. Informational type offer is close for male and female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"event\", hue= \"gender\", data=data)\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.title('Gender distribution as per Event type')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Here are few intresting facts about event type disrtibution among the gender. As we can see that there is huge gap between offer-received and offer-completed. But under offer-completed event, male and female got almost same number of count. Which indicates that male usually ignores the offers mostly or dont pay attention towards it, whereas female pays more attention towards offer. Which seems to be correct in real life as well :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(x= \"event\", hue= \"offer_type\", data=data)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.title('Event type distribution as per offer_type')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Event')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Offer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : As we can see in above chart, that bogo and discount offers get receive in almost same quantity, but discount offers have less views compared to bogo offer, still discount offers are larger then bogo offer in terms of offer-completed category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_received = data[data['offer-received'] == 1].offer_id.value_counts()\n",
    "offer_viewed = data[data['offer-viewed'] == 1].offer_id.value_counts()\n",
    "offer_completed = data[data['offer-completed'] == 1].offer_id.value_counts()\n",
    "\n",
    " \n",
    "print(\"Offer-Received as per types : {}\".format(str(offer_received)))\n",
    "print(\"Offer-Viewed as per types : {}\".format(str(offer_viewed)))\n",
    "print(\"Offer-Completed as per types : {}\".format(str(offer_completed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining three subplots with grid size 1*3 for 1st subplot, with grid size 1*3 for 2nd subplot, with 1*3 grid size for 3rd subplot.\n",
    "plt.subplot(1,3,1)\n",
    "offer_received = data[data['offer-received'] == 1].offer_id.value_counts()\n",
    "offer_received.plot(kind='barh', figsize=(15,5))\n",
    "plt.ylabel('Offer Id')\n",
    "plt.xlabel('count')\n",
    "plt.title('Offer received with Offer Id ');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "offer_viewed = data[data['offer-viewed'] == 1].offer_id.value_counts()\n",
    "offer_viewed.plot(kind='barh' , figsize=(15,5))\n",
    "plt.ylabel('Offer Id')\n",
    "plt.xlabel('count')\n",
    "plt.title('Offer viewed with Offer Id ');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "offer_completed = data[data['offer-completed'] == 1].offer_id.value_counts()\n",
    "offer_completed.plot(kind='barh' , figsize=(15,5))\n",
    "plt.ylabel('Offer Id')\n",
    "plt.xlabel('count')\n",
    "plt.title('Offer completed got with Offer Id ');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: From above subplots we can analyze that all Offer id received equal offers and 60% offer ids did view the offer in terms of good count, rest 40% is on average mark. If we talk about offer completion with respect to offer ids, then its showing considerably good count for all offer ids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am pulling three platforms where offers get shoot up mostly, email, mobile and social platforms.\n",
    "email_received = df1[df1['email'] == 1].offer_id.value_counts()\n",
    "mobile_received = df1[df1['mobile'] == 1].offer_id.value_counts()\n",
    "social_received = df1[df1['social'] == 1].offer_id.value_counts()\n",
    "\n",
    " \n",
    "print(\"email-Received as per types : {}\".format(str(email_received)))\n",
    "print(\"mobile-Viewed as per types : {}\".format(str(mobile_received)))\n",
    "print(\"social-Completed as per types : {}\".format(str(social_received)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining three subplots with grid size 1*3 for 1st subplot, with grid size 1*3 for 2nd subplot, with 1*3 grid size for 3rd subplot.\n",
    "plt.subplot(1,3,1)\n",
    "email = df1[df1['email'] == 1].offer_id.value_counts()\n",
    "email.plot(kind='bar')\n",
    "plt.ylabel('email')\n",
    "plt.xlabel('Offer Id')\n",
    "plt.title('Offer id linked with email ');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "mobile = df1[df1['mobile'] == 1].offer_id.value_counts()\n",
    "mobile.plot(kind='bar' , figsize=(15,5))\n",
    "plt.ylabel('mobile')\n",
    "plt.xlabel('Offer Id')\n",
    "plt.title('Offer id linked with mobile ');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "social = df1[df1['social'] == 1].offer_id.value_counts()\n",
    "social.plot(kind='bar' , figsize=(15,5))\n",
    "plt.ylabel('social')\n",
    "plt.xlabel('Offer Id')\n",
    "plt.title('Offer id linked with social ');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation : As we can see here offers get shoot up on email mostly. or can say first preference for Starbucks. Second its Mobile, and its also very popular medium for shooting offers. Mobile platform is almost equal to email. Social platform is bit low in terms of shooting offers, as its showing 60% compared to email. So may be Starbucks can consider to boost up the offers over social platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build a Machine Learning model to predict response of a customer to the offer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying one hot encoding for gender and offer_type column, its the part of procedure to prepare the Pre Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying gender categories and mapping to existing gender categories\n",
    "genders = {'O': 0, 'M': 1, 'F': 2}\n",
    "data['gender'] = data['gender'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.offer_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classifying offer_type categories and mapping to existing offer_type categories\n",
    "offers = {'bogo': 0, 'discount': 1, 'informational': 2}\n",
    "data['offer_type'] = data['offer_type'].map(offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inorder to create test and train variables for testing and training , we need to look into \"data\" dataframe again and arrange\n",
    "# the columns as per need.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['customer_id', 'event_id' , 'event' , 'became_member_on','offer-completed', 'offer-received','offer-viewed', 'transaction'], axis=1)\n",
    "Y = data['event_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['income'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['income'].fillna(X['income'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['income'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape , Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name =['offer recieved', 'offer viewed', 'transaction', 'offer completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into test and train sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.income = std.fit_transform(X_train.income.values.reshape(-1, 1))\n",
    "X_train.age = std.fit_transform(X_train.age.values.reshape(-1, 1))\n",
    "\n",
    "X_train.reset_index(inplace=True)\n",
    "X_train = X_train.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.income = std.fit_transform(X_train.income.values.reshape(-1, 1))\n",
    "X_train.age = std.fit_transform(X_train.age.values.reshape(-1, 1))\n",
    "\n",
    "X_train.reset_index(inplace=True)\n",
    "X_train = X_train.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 100\n",
    "NUM_COLS = 100\n",
    "ann.add(keras.layers.Dense(6, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
    "#ann.add(Dropout(0.5))\n",
    "ann.add(keras.layers.Dense(6, activation='relu'))\n",
    "#ann.add(Dropout(0.25))\n",
    "ann.add(keras.layers.Dense(4, activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', \n",
    "            loss = 'sparse_categorical_crossentropy', \n",
    "            metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_history = ann.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=100)\n",
    "#plot_losses = livelossplot.PlotLossesKeras()\n",
    "\n",
    " \n",
    "\n",
    "#ann.fit(X_train, y_train, epochs=15, batch_size=100, verbose=1)\n",
    "\n",
    "#ann.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1, callbacks=[], validation_split=0.0, validation_data=(X_test, y_test), shuffle=True, class_weight=None, sample_weight=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
